{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Psycological Analysis**\n",
        "This model aims at predicting the psycological problem that a patient faces according to the information provided by the patient. In the models I used three input parameters on the basis of which the model is predicting the problem faced by the patient. These parameters are:-\n",
        "\n",
        "\n",
        "1.   Age\n",
        "2.   Gender\n",
        "3.   Problem Description\n",
        "\n",
        "I used method of NLP and vectorization of data an predict patterns based on data to predict the problem faced by the patient.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2z8aWmxX2SHs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qIWOr687E1Ie"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from sklearn.model_selection import GridSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AkjRcZB0Qxs_",
        "outputId": "7bf3678d-2d07-404a-8dec-f319109867f0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.set_printoptions(threshold=np.inf)"
      ],
      "metadata": {
        "id": "LzflWpQP0_Zo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "file = pd.read_csv(\"preprocessed_data.csv\")\n",
        "sw = stopwords.words('english') + ['.', '?', ',', \"'\", '-', \" \"]\n",
        "port_stem = PorterStemmer()\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "cv = CountVectorizer()\n",
        "\n",
        "data = list(file[\"Problem_description\"])"
      ],
      "metadata": {
        "id": "fjiUgh-u0hlU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text preprocessing\n",
        "def extract_sents(data):\n",
        "    extracted_sents = []\n",
        "    for sent in data:\n",
        "        doc = nlp(sent)\n",
        "        lemmatized_words = [token.lemma_ for token in doc if token.text.lower() not in sw]\n",
        "        extracted_sents.append(\" \".join(lemmatized_words))\n",
        "    return list(extracted_sents)\n",
        "\n",
        "extract_sents = extract_sents(data)"
      ],
      "metadata": {
        "id": "3dmQPO4L1TLJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the data\n",
        "vc = cv.fit_transform(extract_sents)\n",
        "vectorized_sens = list(vc.toarray())\n",
        "np_vectorized_sens = np.array(vectorized_sens)\n",
        "\n",
        "# ... (Code to save vocabulary omitted for brevity)"
      ],
      "metadata": {
        "id": "BsVAB97j1W_c"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare features and labels\n",
        "data_gender = list(file[\"Gender\"])\n",
        "cv_Gender = CountVectorizer()\n",
        "vc_Gender = cv_Gender.fit_transform(data_gender)\n",
        "np_Gender = np.array(vc_Gender.toarray()[:, 1])\n",
        "\n",
        "le = LabelEncoder()\n",
        "data_problem = list(file[\"psychological_catehory\"])\n",
        "fit = np.array(le.fit_transform(data_problem))\n",
        "classes = le.classes_\n",
        "np.save(\"classes.npy\", classes)\n",
        "\n",
        "data_age = np.array(file[\"Age\"])\n",
        "final_data = np.hstack((data_age.reshape(-1, 1), np_Gender.reshape(-1, 1), np_vectorized_sens, fit.reshape(-1, 1)))\n",
        "np.save(\"final_data.npy\", final_data)"
      ],
      "metadata": {
        "id": "QEIIX5nL1W8X"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Oversample the minority class\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X, y = final_data[:, :-1], final_data[:, -1]\n",
        "X_resampled, y_resampled = ros.fit_resample(X, y)"
      ],
      "metadata": {
        "id": "vGdMFGm51W4y"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "3737pwvZ1gZQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the models\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=1000, random_state=42, max_depth=8),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42),\n",
        "    'SVM': SVC(kernel='linear', random_state=42)\n",
        "}"
      ],
      "metadata": {
        "id": "jYWs7HNj1gWe"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning using GridSearchCV\n",
        "param_grids = {\n",
        "    'Random Forest': {'n_estimators': [500, 1000, 1500], 'max_depth': [4, 8, 12]},\n",
        "    'Logistic Regression': {'C': [0.1, 1, 10]},\n",
        "    'SVM': {'C': [0.1, 1, 10]}\n",
        "}"
      ],
      "metadata": {
        "id": "0y5Q4dFP1gTa"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_models = {}\n",
        "for name, model in models.items():\n",
        "    print(f\"Tuning hyperparameters for {name}...\")\n",
        "    grid_search = GridSearchCV(model, param_grids[name], cv=5, scoring='accuracy', n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    best_models[name] = grid_search.best_estimator_\n",
        "    print(f\"Best hyperparameters: {grid_search.best_params_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hO55Uai91gQl",
        "outputId": "db601b48-ff86-4b15-9fc5-d8d676736a20"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tuning hyperparameters for Random Forest...\n",
            "Best hyperparameters: {'max_depth': 12, 'n_estimators': 1000}\n",
            "Tuning hyperparameters for Logistic Regression...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'C': 0.1}\n",
            "Tuning hyperparameters for SVM...\n",
            "Best hyperparameters: {'C': 0.1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the models\n",
        "for name, model in best_models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f\"{name} Accuracy: {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5ALugJW1rCd",
        "outputId": "d51bc2e6-a5e1-4397-9405-aef37b40a6e1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Forest Accuracy: 0.9861495844875346\n",
            "Logistic Regression Accuracy: 0.9889196675900277\n",
            "SVM Accuracy: 0.9916897506925207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the models\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9w5tUHz1q_R",
        "outputId": "eac056bb-4d21-4d72-a39b-5b3e45a08327"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Text_Processing:\n",
        "    def __init__(self, age, gender, problem_description):\n",
        "        self.age = age\n",
        "        self.gender = gender\n",
        "        self.problem_description = problem_description\n",
        "\n",
        "    def extract_sents(self):\n",
        "        extracted_sents = []\n",
        "        for sent in self.problem_description:\n",
        "            doc = nlp(sent)\n",
        "            lemmatized_words = [token.lemma_ for token in doc if token.text.lower() not in sw]\n",
        "            extracted_sents.append(\" \".join(lemmatized_words))\n",
        "        return list(extracted_sents)\n",
        "\n",
        "    def vectorize_sents(self):\n",
        "        extract_sents = self.extract_sents()\n",
        "        vc = cv.transform(extract_sents)\n",
        "        vectorized_sens = list(vc.toarray())\n",
        "        np_vectorized_sens = np.array(vectorized_sens)\n",
        "        return np_vectorized_sens\n",
        "\n",
        "    def processing(self):\n",
        "        # Processing Gender\n",
        "        if self.gender.lower() == \"male\":\n",
        "            gender = 0\n",
        "        else:\n",
        "            gender = 1\n",
        "        gender = np.array([gender]).reshape(-1, 1)\n",
        "        age = np.array([self.age]).reshape(-1, 1)\n",
        "        vectorized_sens = self.vectorize_sents()\n",
        "        final_stack = np.hstack((age, gender, vectorized_sens))\n",
        "        return final_stack"
      ],
      "metadata": {
        "id": "dVj_NQ511q8Q"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get user input\n",
        "age = int(input(\"Enter your age: \"))\n",
        "gender = input(\"Enter your gender: \")\n",
        "problem = input(\"Enter your problem: \")\n",
        "\n",
        "# Process user input\n",
        "obj = Text_Processing(age, gender, [problem])\n",
        "value = obj.processing()\n",
        "\n",
        "# Predict the psychological problem\n",
        "problem_value = models['SVM'].predict(value)\n",
        "\n",
        "# Convert problem_value to original form\n",
        "classes = np.load(\"classes.npy\")\n",
        "problem_value = classes[problem_value[0]]\n",
        "\n",
        "print(f\"Your problem is related to {problem_value}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6h6Xx6RL1q5G",
        "outputId": "429cd6ba-f148-42b3-c982-cb9ab2c80d0a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your age: 27\n",
            "Enter your gender: m\n",
            "Enter your problem: I am having trouble focusing on the job\n",
            "Your problem is related to Anxiety\n"
          ]
        }
      ]
    }
  ]
}